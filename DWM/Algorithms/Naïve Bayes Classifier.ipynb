{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNrqvWCQeQpNCL0ADyCWciN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VqeWCJMi2ZRb","executionInfo":{"status":"ok","timestamp":1761521348455,"user_tz":-330,"elapsed":3255,"user":{"displayName":"James Lewis","userId":"11797824913672368875"}},"outputId":"54ff7ede-a021-4908-eb10-dd3efb85455b"},"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","           NAÏVE BAYES CLASSIFIER IMPLEMENTATION\n","======================================================================\n","\n","Step 1: Loading Dataset...\n","Dataset: Iris Flower Dataset\n","Total samples: 150\n","Number of features: 4\n","Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n","Classes: ['setosa' 'versicolor' 'virginica']\n","\n","First 5 samples of dataset:\n","   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n","0                5.1               3.5                1.4               0.2   \n","1                4.9               3.0                1.4               0.2   \n","2                4.7               3.2                1.3               0.2   \n","3                4.6               3.1                1.5               0.2   \n","4                5.0               3.6                1.4               0.2   \n","\n","   target  \n","0       0  \n","1       0  \n","2       0  \n","3       0  \n","4       0  \n","\n","----------------------------------------------------------------------\n","Step 2: Splitting Dataset (70% Train, 30% Test)...\n","Training set size: 105 samples\n","Testing set size: 45 samples\n","\n","----------------------------------------------------------------------\n","Step 3: Training Gaussian Naïve Bayes Classifier...\n","Training completed successfully!\n","\n","----------------------------------------------------------------------\n","Step 4: Making Predictions on Test Data...\n","Predictions made successfully!\n","\n","Sample predictions (first 10):\n","Actual:    [1 0 2 1 1 0 1 2 1 1]\n","Predicted: [1 0 2 1 1 0 1 2 1 1]\n","\n","----------------------------------------------------------------------\n","Step 5: Model Evaluation\n","----------------------------------------------------------------------\n","\n","✓ Accuracy: 97.78%\n","\n","✓ Confusion Matrix:\n","[[19  0  0]\n"," [ 0 12  1]\n"," [ 0  0 13]]\n","\n","Confusion Matrix Explanation:\n","   Columns: Predicted Classes\n","   Rows: Actual Classes\n","\n","✓ Classification Report:\n","              precision    recall  f1-score   support\n","\n","      setosa       1.00      1.00      1.00        19\n","  versicolor       1.00      0.92      0.96        13\n","   virginica       0.93      1.00      0.96        13\n","\n","    accuracy                           0.98        45\n","   macro avg       0.98      0.97      0.97        45\n","weighted avg       0.98      0.98      0.98        45\n","\n","\n","----------------------------------------------------------------------\n","Step 6: Testing with New Sample Data\n","----------------------------------------------------------------------\n","\n","New Samples:\n","Sample 1: [5.1, 3.5, 1.4, 0.2]\n","Sample 2: [6.7, 3.0, 5.2, 2.3]\n","\n","Predictions:\n","\n","Sample 1: [5.1, 3.5, 1.4, 0.2]\n","Predicted Class: setosa\n","Prediction Probabilities:\n","  setosa: 100.00%\n","  versicolor: 0.00%\n","  virginica: 0.00%\n","\n","Sample 2: [6.7, 3.0, 5.2, 2.3]\n","Predicted Class: virginica\n","Prediction Probabilities:\n","  setosa: 0.00%\n","  versicolor: 0.00%\n","  virginica: 100.00%\n","\n","======================================================================\n","                          SUMMARY\n","======================================================================\n","\n","✓ Algorithm Used: Gaussian Naïve Bayes\n","✓ Dataset: Iris (150 samples, 4 features, 3 classes)\n","✓ Training Samples: 105\n","✓ Testing Samples: 45\n","✓ Model Accuracy: 97.78%\n","✓ Status: Successfully Classified!\n","\n","======================================================================\n","KEY CONCEPTS:\n","======================================================================\n","\n","1. Naïve Bayes Theorem:\n","   P(Class|Features) = P(Features|Class) * P(Class) / P(Features)\n","\n","2. Assumption:\n","   All features are independent (Naïve assumption)\n","\n","3. Gaussian Distribution:\n","   Used for continuous numerical features\n","   P(x) = (1/√(2πσ²)) * e^(-(x-μ)²/(2σ²))\n","\n","4. Advantages:\n","   • Fast and efficient\n","   • Works well with small datasets\n","   • Good for multi-class classification\n","   • Requires less training data\n","\n","5. Disadvantages:\n","   • Assumes feature independence (rarely true in real world)\n","   • Can be outperformed by complex models\n","   • Zero frequency problem in categorical data\n","\n","======================================================================\n","           IMPLEMENTATION COMPLETED SUCCESSFULLY!\n","======================================================================\n"]}],"source":["\"\"\"\n","Naïve Bayes Classifier Implementation\n","Student Practical Exam - Data Warehouse and Mining\n","\"\"\"\n","\n","# Import required libraries\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n","import pandas as pd\n","import numpy as np\n","\n","print(\"=\"*70)\n","print(\"           NAÏVE BAYES CLASSIFIER IMPLEMENTATION\")\n","print(\"=\"*70)\n","\n","# Step 1: Load Dataset\n","print(\"\\nStep 1: Loading Dataset...\")\n","iris = load_iris()\n","X = iris.data  # Features\n","y = iris.target  # Target labels\n","\n","print(f\"Dataset: Iris Flower Dataset\")\n","print(f\"Total samples: {X.shape[0]}\")\n","print(f\"Number of features: {X.shape[1]}\")\n","print(f\"Feature names: {iris.feature_names}\")\n","print(f\"Classes: {iris.target_names}\")\n","\n","# Display first 5 samples\n","print(\"\\nFirst 5 samples of dataset:\")\n","df = pd.DataFrame(X, columns=iris.feature_names)\n","df['target'] = y\n","print(df.head())\n","\n","# Step 2: Split Dataset into Training and Testing Sets\n","print(\"\\n\" + \"-\"*70)\n","print(\"Step 2: Splitting Dataset (70% Train, 30% Test)...\")\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","\n","print(f\"Training set size: {X_train.shape[0]} samples\")\n","print(f\"Testing set size: {X_test.shape[0]} samples\")\n","\n","# Step 3: Create and Train Naïve Bayes Classifier\n","print(\"\\n\" + \"-\"*70)\n","print(\"Step 3: Training Gaussian Naïve Bayes Classifier...\")\n","\n","# Create Gaussian Naïve Bayes model\n","model = GaussianNB()\n","\n","# Train the model\n","model.fit(X_train, y_train)\n","print(\"Training completed successfully!\")\n","\n","# Step 4: Make Predictions on Test Data\n","print(\"\\n\" + \"-\"*70)\n","print(\"Step 4: Making Predictions on Test Data...\")\n","\n","y_pred = model.predict(X_test)\n","\n","print(\"Predictions made successfully!\")\n","print(f\"\\nSample predictions (first 10):\")\n","print(f\"Actual:    {y_test[:10]}\")\n","print(f\"Predicted: {y_pred[:10]}\")\n","\n","# Step 5: Evaluate Model Performance\n","print(\"\\n\" + \"-\"*70)\n","print(\"Step 5: Model Evaluation\")\n","print(\"-\"*70)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"\\n✓ Accuracy: {accuracy * 100:.2f}%\")\n","\n","# Confusion Matrix\n","print(\"\\n✓ Confusion Matrix:\")\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)\n","print(\"\\nConfusion Matrix Explanation:\")\n","print(f\"   Columns: Predicted Classes\")\n","print(f\"   Rows: Actual Classes\")\n","\n","# Classification Report\n","print(\"\\n✓ Classification Report:\")\n","print(classification_report(y_test, y_pred, target_names=iris.target_names))\n","\n","# Step 6: Test with New Sample Data\n","print(\"\\n\" + \"-\"*70)\n","print(\"Step 6: Testing with New Sample Data\")\n","print(\"-\"*70)\n","\n","# New sample (hypothetical flower measurements)\n","new_samples = [\n","    [5.1, 3.5, 1.4, 0.2],  # Likely Setosa\n","    [6.7, 3.0, 5.2, 2.3],  # Likely Virginica\n","]\n","\n","print(\"\\nNew Samples:\")\n","for i, sample in enumerate(new_samples, 1):\n","    print(f\"Sample {i}: {sample}\")\n","\n","# Predict class\n","predictions = model.predict(new_samples)\n","\n","# Predict probabilities\n","probabilities = model.predict_proba(new_samples)\n","\n","print(\"\\nPredictions:\")\n","for i, (sample, pred, prob) in enumerate(zip(new_samples, predictions, probabilities), 1):\n","    print(f\"\\nSample {i}: {sample}\")\n","    print(f\"Predicted Class: {iris.target_names[pred]}\")\n","    print(f\"Prediction Probabilities:\")\n","    for j, class_name in enumerate(iris.target_names):\n","        print(f\"  {class_name}: {prob[j]*100:.2f}%\")\n","\n","# Summary\n","print(\"\\n\" + \"=\"*70)\n","print(\"                          SUMMARY\")\n","print(\"=\"*70)\n","print(f\"\\n✓ Algorithm Used: Gaussian Naïve Bayes\")\n","print(f\"✓ Dataset: Iris (150 samples, 4 features, 3 classes)\")\n","print(f\"✓ Training Samples: {len(X_train)}\")\n","print(f\"✓ Testing Samples: {len(X_test)}\")\n","print(f\"✓ Model Accuracy: {accuracy * 100:.2f}%\")\n","print(f\"✓ Status: Successfully Classified!\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"KEY CONCEPTS:\")\n","print(\"=\"*70)\n","print(\"\"\"\n","1. Naïve Bayes Theorem:\n","   P(Class|Features) = P(Features|Class) * P(Class) / P(Features)\n","\n","2. Assumption:\n","   All features are independent (Naïve assumption)\n","\n","3. Gaussian Distribution:\n","   Used for continuous numerical features\n","   P(x) = (1/√(2πσ²)) * e^(-(x-μ)²/(2σ²))\n","\n","4. Advantages:\n","   • Fast and efficient\n","   • Works well with small datasets\n","   • Good for multi-class classification\n","   • Requires less training data\n","\n","5. Disadvantages:\n","   • Assumes feature independence (rarely true in real world)\n","   • Can be outperformed by complex models\n","   • Zero frequency problem in categorical data\n","\"\"\")\n","\n","print(\"=\"*70)\n","print(\"           IMPLEMENTATION COMPLETED SUCCESSFULLY!\")\n","print(\"=\"*70)\n"]},{"cell_type":"markdown","source":["Option 2"],"metadata":{"id":"QFEsVTQE2sZ8"}},{"cell_type":"code","source":["# Naive Bayes Classifier Implementation in Python\n","# Author: James Lewis\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","# Step 1: Sample Dataset (Example)\n","# Each record = [feature1, feature2, ..., label]\n","# Here we’ll use a simple dataset for demonstration\n","\n","# Features: [age, income]\n","# Labels: 0 = No, 1 = Yes (e.g., “buys_computer”)\n","X = [[25, 40000], [35, 60000], [45, 80000], [20, 20000], [50, 100000]]\n","y = [0, 1, 1, 0, 1]\n","\n","# Step 2: Split data into training and testing\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Step 3: Create and train the model\n","model = GaussianNB()\n","model.fit(X_train, y_train)\n","\n","# Step 4: Predict using test data\n","y_pred = model.predict(X_test)\n","\n","# Step 5: Evaluate\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"Predictions:\", y_pred)\n","print(\"Accuracy:\", accuracy)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DSPf5Xdj2vO6","executionInfo":{"status":"ok","timestamp":1761521396281,"user_tz":-330,"elapsed":8,"user":{"displayName":"James Lewis","userId":"11797824913672368875"}},"outputId":"93c52283-1bd8-4414-bfbe-2ace764930ec"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Predictions: [0]\n","Accuracy: 0.0\n"]}]},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score\n","\n","# Load data\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# Split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n","\n","# Train\n","model = GaussianNB()\n","model.fit(X_train, y_train)\n","\n","# Predict\n","y_pred = model.predict(X_test)\n","\n","# Evaluate\n","print(f\"Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n","\n","# Test new sample\n","new = [[5.1, 3.5, 1.4, 0.2]]\n","print(f\"Prediction: {iris.target_names[model.predict(new)[0]]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PzZ1UORy3Nhp","executionInfo":{"status":"ok","timestamp":1761521493691,"user_tz":-330,"elapsed":24,"user":{"displayName":"James Lewis","userId":"11797824913672368875"}},"outputId":"915b8b2f-f18c-42f9-f275-71207e04a14e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 95.56%\n","Prediction: setosa\n"]}]},{"cell_type":"code","source":["\"\"\"\n","Naïve Bayes Classifier - Simple Implementation\n","\"\"\"\n","\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","\n","print(\"=\"*60)\n","print(\"      NAÏVE BAYES CLASSIFIER\")\n","print(\"=\"*60)\n","\n","# Step 1: Load Data\n","print(\"\\n1. Loading Iris Dataset...\")\n","iris = load_iris()\n","X = iris.data\n","y = iris.target\n","\n","print(f\"   Total Samples: {len(X)}\")\n","print(f\"   Features: {iris.feature_names}\")\n","print(f\"   Classes: {iris.target_names}\")\n","\n","# Step 2: Split Data\n","print(\"\\n2. Splitting Data (70-30)...\")\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=42\n",")\n","print(f\"   Training: {len(X_train)}, Testing: {len(X_test)}\")\n","\n","# Step 3: Train Model\n","print(\"\\n3. Training Naïve Bayes...\")\n","model = GaussianNB()\n","model.fit(X_train, y_train)\n","print(\"   ✓ Training Complete!\")\n","\n","# Step 4: Make Predictions\n","print(\"\\n4. Making Predictions...\")\n","y_pred = model.predict(X_test)\n","print(f\"   First 5 Predictions: {y_pred[:5]}\")\n","print(f\"   Actual Values:       {y_test[:5]}\")\n","\n","# Step 5: Evaluate\n","print(\"\\n5. Model Evaluation\")\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"   Accuracy: {accuracy*100:.2f}%\")\n","\n","print(\"\\n   Confusion Matrix:\")\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)\n","\n","# Step 6: Test New Sample\n","print(\"\\n6. Testing New Sample\")\n","new_sample = [[5.1, 3.5, 1.4, 0.2]]\n","prediction = model.predict(new_sample)\n","probability = model.predict_proba(new_sample)\n","\n","print(f\"   Sample: {new_sample[0]}\")\n","print(f\"   Predicted: {iris.target_names[prediction[0]]}\")\n","print(f\"   Probabilities:\")\n","for i, prob in enumerate(probability[0]):\n","    print(f\"      {iris.target_names[i]}: {prob*100:.2f}%\")\n","\n","print(\"\\n\" + \"=\"*60)\n","print(\"✓ COMPLETE! Accuracy: {:.2f}%\".format(accuracy*100))\n","print(\"=\"*60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OCLJd5_n3PHV","executionInfo":{"status":"ok","timestamp":1761521536227,"user_tz":-330,"elapsed":35,"user":{"displayName":"James Lewis","userId":"11797824913672368875"}},"outputId":"29465a1c-663b-470b-9c48-43b7443f3c43"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["============================================================\n","      NAÏVE BAYES CLASSIFIER\n","============================================================\n","\n","1. Loading Iris Dataset...\n","   Total Samples: 150\n","   Features: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n","   Classes: ['setosa' 'versicolor' 'virginica']\n","\n","2. Splitting Data (70-30)...\n","   Training: 105, Testing: 45\n","\n","3. Training Naïve Bayes...\n","   ✓ Training Complete!\n","\n","4. Making Predictions...\n","   First 5 Predictions: [1 0 2 1 1]\n","   Actual Values:       [1 0 2 1 1]\n","\n","5. Model Evaluation\n","   Accuracy: 97.78%\n","\n","   Confusion Matrix:\n","[[19  0  0]\n"," [ 0 12  1]\n"," [ 0  0 13]]\n","\n","6. Testing New Sample\n","   Sample: [5.1, 3.5, 1.4, 0.2]\n","   Predicted: setosa\n","   Probabilities:\n","      setosa: 100.00%\n","      versicolor: 0.00%\n","      virginica: 0.00%\n","\n","============================================================\n","✓ COMPLETE! Accuracy: 97.78%\n","============================================================\n"]}]}]}